{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install nb_conda_kernels ipywidgets -c conda-forge -n base -y\n",
    "! conda env create -f environment.yml\n",
    "! conda env update -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from variable_width_resnet import resnet10vw\n",
    "import json\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/nobackup/data/celebA\"\n",
    "\n",
    "label = \"Blond_Hair\"\n",
    "spurious = \"Male\"\n",
    "\n",
    "batch_size_teacher = 1024\n",
    "batch_size_student = 2048\n",
    "max_workers = 64\n",
    "epochs = 60\n",
    "max_width = 96\n",
    "folds = 10\n",
    "temperature = 3.\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.has_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(f\"{data_dir}/list_attr_celeba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReweightedLoss(group_fractions):\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    \n",
    "    def loss(logits, labels, groups):\n",
    "        losses = cross_entropy(logits, labels)\n",
    "        \n",
    "        for group_id in range(group_fractions.shape[0]):\n",
    "            losses[groups == group_id] /= group_fractions[group_id]\n",
    "            \n",
    "        return losses.mean()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def DistillationLossReweighted(temperature, group_fractions):\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def loss(student_logits, teacher_logits, target, groups):\n",
    "        last_dim = len(student_logits.shape) - 1\n",
    "        \n",
    "        p_t = nn.functional.softmax(teacher_logits/temperature, dim=last_dim)\n",
    "        log_p_s = nn.functional.log_softmax(student_logits/temperature, dim=last_dim)\n",
    "        \n",
    "        losses = cross_entropy(student_logits, target) - (p_t * log_p_s).sum(dim=last_dim)\n",
    "        \n",
    "        for group_id in range(group_fractions.shape[0]):\n",
    "            losses[groups == group_id] /= group_fractions[group_id]\n",
    "        \n",
    "        return losses.mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, name=\"\"):\n",
    "    total_correct = torch.zeros(2,2)\n",
    "    total_items = torch.zeros(2,2)\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, attrs in iter(loader):\n",
    "            target = attrs[..., 1]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            out = model(data)\n",
    "            \n",
    "            correct = (out.argmax(axis=1) == target).to(\"cpu\")\n",
    "            batch_size = correct.shape[0]\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                bin_idx = list(attrs[batch_idx])\n",
    "                \n",
    "                total_correct[bin_idx] += correct[batch_idx]\n",
    "                total_items[bin_idx] += 1\n",
    "                \n",
    "    model.train(training)\n",
    "                \n",
    "    group_accuracies = total_correct / total_items\n",
    "                \n",
    "    print(f\"{name} set results:\")\n",
    "    print(f\"Average case accuracy: {total_correct.sum()}/{total_items.sum()}={total_correct.sum()/total_items.sum()}\\n\")\n",
    "    \n",
    "    status = lambda x: \" not \" if x == 0 else \" \"\n",
    "    \n",
    "    def from_flat_idx(x):\n",
    "        idx0 = 1 if x > 1 else 0\n",
    "        idx1 = x - 2 * idx0\n",
    "        \n",
    "        return idx0, idx1\n",
    "    \n",
    "    worst = from_flat_idx(group_accuracies.argmin())\n",
    "    \n",
    "    print(f\"Worst case group:{status(worst[0])}{spurious.lower()} [spurious] and{status(worst[1])}{label.lower()} [target]\")\n",
    "    print(f\"Worst case accuracy: {total_correct[worst]}/{total_items[worst]}={group_accuracies[worst]}\\n\")\n",
    "    \n",
    "#     best = from_flat_idx(group_accuracies.argmax())\n",
    "#     print(f\"Best case group:{status(best[0])}{spurious.lower()} [spurious] and{status(best[1])}{label.lower()} [target]\")\n",
    "#     print(f\"Best case accuracy: {total_correct[best]}/{total_items[best]}={group_accuracies[best]}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"worst\": {\n",
    "            \"correct\": total_correct[worst].item(),\n",
    "            \"total\": total_items[worst].item(),\n",
    "        },\n",
    "        \"average\": {\n",
    "            \"correct\": total_correct.sum().item(),\n",
    "            \"total\": total_items.sum().item(),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebAFairnessDataset(torchvision.datasets.VisionDataset):\n",
    "    def __init__(self, \n",
    "                 split,\n",
    "                 attribute_df=attributes, \n",
    "                 is_training=True, \n",
    "                 augment=False,\n",
    "                 spurious=spurious, \n",
    "                 label=label,\n",
    "                 root=f\"{data_dir}/img_align_celeba/\"):        \n",
    "        df = attribute_df.loc[split]\n",
    "        \n",
    "        self.files = root + df.image_id.values\n",
    "        \n",
    "        self.attrs = df[[spurious, label]].values\n",
    "        \n",
    "        # Negative cases are labeled 0 instead of -1\n",
    "        self.attrs[self.attrs < 0] = 0\n",
    "        \n",
    "        # Overparameterization paper transform:\n",
    "        orig_w = 178\n",
    "        orig_h = 218\n",
    "        orig_min_dim = min(orig_w, orig_h)\n",
    "        target_resolution = (224, 224)\n",
    "\n",
    "        if (not is_training) or (not augment):\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.CenterCrop(orig_min_dim),\n",
    "                transforms.Resize(target_resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            # Orig aspect ratio is 0.81, so we don't squish it in that direction any more\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(\n",
    "                    target_resolution,\n",
    "                    scale=(0.7, 1.0),\n",
    "                    ratio=(1.0, 1.3333333333333333),\n",
    "                    interpolation=2),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = PIL.Image.open(self.files[idx])\n",
    "        attrs = self.attrs[idx]\n",
    "        \n",
    "        \n",
    "        return [\n",
    "            self.transform(img),\n",
    "            torch.Tensor(attrs).long()\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.files.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permuted_indices = torch.randperm(attributes.index.values.shape[0])\n",
    "# torch.save(permuted_indices, \"permutation_cross_validation.pt\")\n",
    "permuted_indices = torch.load(\"permutation_cross_validation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold, permuted_indices=permuted_indices, augment_teacher=False, augment_student=True, load=False):\n",
    "    message = f\"Starting fold {fold+1} of {folds}\"\n",
    "    print(message)\n",
    "    print(\"-\" * len(message))\n",
    "    \n",
    "    datapoints = permuted_indices.numel()\n",
    "    indices = torch.arange(datapoints)\n",
    "    test_size = ceil(datapoints/folds)\n",
    "    test_end = ceil(datapoints * (fold + 1) / folds)\n",
    "    test_start = test_end - test_size\n",
    "    \n",
    "    test = permuted_indices[test_start:test_end]\n",
    "    train = permuted_indices[(indices > test_end) | (indices < test_start)]\n",
    "    training = CelebAFairnessDataset(train, augment=augment_teacher)\n",
    "    \n",
    "    teacher_train_loader = torch.utils.data.DataLoader(training,\n",
    "                                               num_workers=max_workers,\n",
    "                                               batch_size=batch_size_teacher,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True)\n",
    "    student_train_loader = torch.utils.data.DataLoader(CelebAFairnessDataset(train, augment=augment_student),\n",
    "                                               num_workers=max_workers,\n",
    "                                               batch_size=batch_size_student,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(CelebAFairnessDataset(test),\n",
    "                                               num_workers=max_workers,\n",
    "                                               batch_size=batch_size_student,\n",
    "                                               shuffle=False,\n",
    "                                               pin_memory=True)\n",
    "    \n",
    "    test_results = lambda model: evaluate(model, test_loader, name=\"Test\")\n",
    "    \n",
    "    attrs = torch.Tensor(training.attrs).long()\n",
    "    group_fractions = torch.bincount(attrs[..., 0] + attrs[...,1]*2).float()\n",
    "    group_fractions /= group_fractions.sum()\n",
    "    \n",
    "    if load:\n",
    "        start_epoch, is_teacher, last_fold = torch.load(\"state.pt\")\n",
    "    \n",
    "    if load and last_fold == fold:\n",
    "        print(\"restored state\")\n",
    "        teacher = torch.load(\"teacher_model.pt\").to(device)\n",
    "        student = torch.load(\"student_model.pt\").to(device)\n",
    "        student_no_distillation = torch.load(\"student_no_distillation.pt\").to(device)\n",
    "        \n",
    "        optimizer_teacher = torch.load(\"teacher_optimizer.pt\")\n",
    "        optimizer_student = torch.load(\"student_optimizer.pt\")\n",
    "        optimizer_no_distillation = torch.load(\"student_no_distillation_optimizer.pt\")\n",
    "    else:\n",
    "        teacher = nn.DataParallel(resnet10vw(96, num_classes=2)).to(device)\n",
    "        student =  nn.DataParallel(resnet10vw(10, num_classes=2)).to(device)\n",
    "        student_no_distillation = nn.DataParallel(resnet10vw(10, num_classes=2)).to(device)\n",
    "\n",
    "        optimizer_teacher = optim.Adam(teacher.parameters())\n",
    "        optimizer_student = optim.Adam(student.parameters())\n",
    "        optimizer_no_distillation = optim.Adam(student_no_distillation.parameters())\n",
    "        \n",
    "        start_epoch = -1\n",
    "        is_teacher = True\n",
    "    \n",
    "    def save(epoch, is_teacher):\n",
    "        torch.save(teacher, \"teacher_model.pt\")\n",
    "        torch.save(student, \"student_model.pt\")\n",
    "        torch.save(student_no_distillation, \"student_no_distillation.pt\")\n",
    "        \n",
    "        torch.save(optimizer_teacher, \"teacher_optimizer.pt\")\n",
    "        torch.save(optimizer_student, \"student_optimizer.pt\")\n",
    "        torch.save(optimizer_no_distillation, \"student_no_distillation_optimizer.pt\")\n",
    "               \n",
    "        torch.save([epoch, is_teacher, fold], \"state.pt\")\n",
    "\n",
    "    if is_teacher:\n",
    "        # Train teacher   \n",
    "        print(\"\\nTraining teacher model\")\n",
    "                                             \n",
    "        criterion = ReweightedLoss(group_fractions)\n",
    "\n",
    "        teacher.train()\n",
    "        \n",
    "        for epoch in range(start_epoch+1, epochs):\n",
    "            epoch_loss = 0.\n",
    "            start = time()\n",
    "\n",
    "            for data, attrs in iter(teacher_train_loader):\n",
    "                target = attrs[..., 1] # Spurious label is index 0\n",
    "                groups = attrs[..., 0] + attrs[..., 1] * 2\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                out = teacher(data)\n",
    "                loss = criterion(out, target, groups)\n",
    "                loss.backward()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                optimizer_teacher.step()      \n",
    "                optimizer_teacher.zero_grad()\n",
    "\n",
    "            end = time()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}: total loss {epoch_loss} in {end - start} seconds\")\n",
    "                save(epoch, True)\n",
    "                \n",
    "        start_epoch = -1\n",
    "\n",
    "    # Train student\n",
    "    print(\"\\nTraining student models\")\n",
    "    \n",
    "    criterion = DistillationLossReweighted(temperature, group_fractions)\n",
    "    criterion_no_distillation = ReweightedLoss(group_fractions)\n",
    "            \n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "\n",
    "    for epoch in range(start_epoch+1, epochs):\n",
    "        epoch_loss = 0.\n",
    "        epoch_loss_no_distillation = 0.\n",
    "        start = time()\n",
    "\n",
    "        for data, attrs in iter(student_train_loader):\n",
    "            target = attrs[..., 1] # Spurious label is index 0\n",
    "            groups = attrs[..., 0] + attrs[..., 1] * 2\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            out_student = student(data)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_teacher = teacher(data)\n",
    "\n",
    "            loss = criterion(out_student, out_teacher, target, groups)\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer_student.step()\n",
    "            optimizer_student.zero_grad()\n",
    "            \n",
    "            out = student_no_distillation(data)\n",
    "            loss = criterion_no_distillation(out, target, groups)\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss_no_distillation += loss.item()\n",
    "\n",
    "            optimizer_no_distillation.step()      \n",
    "            optimizer_no_distillation.zero_grad()\n",
    "\n",
    "        end = time()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: total loss {epoch_loss} (with distillation) {epoch_loss_no_distillation} (without distillation) in {end - start} seconds\")\n",
    "            save(epoch, False)\n",
    "            \n",
    "    # Evaluate\n",
    "    print(\"\\n Teacher results:\")\n",
    "    teacher_results = test_results(teacher)\n",
    "    print(\"\\n Student results:\")\n",
    "    student_results = test_results(student)\n",
    "    print(\"\\n Student results (without distillation):\")\n",
    "    student_no_distillation_results = test_results(student_no_distillation)\n",
    "    \n",
    "    return {\n",
    "        \"teacher\": teacher_results,\n",
    "        \"student\": student_results,\n",
    "        \"student_no_distillation\": student_no_distillation_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 1 of 10\n",
      "---------------------\n",
      "\n",
      "Training teacher model\n",
      "Epoch 10: total loss 142.2655338048935 in 87.93975853919983 seconds\n",
      "Epoch 20: total loss 82.69816225767136 in 85.7453465461731 seconds\n",
      "Epoch 30: total loss 36.644175469875336 in 87.8168773651123 seconds\n",
      "Epoch 40: total loss 19.08888928219676 in 84.54312634468079 seconds\n",
      "Epoch 50: total loss 165.0139730423689 in 85.87446737289429 seconds\n",
      "Epoch 60: total loss 1.4040055001387373 in 88.7175042629242 seconds\n",
      "\n",
      "Training student model\n",
      "Epoch 10: total loss 152.24468314647675 in 89.62184691429138 seconds\n",
      "Epoch 20: total loss 163.03148370981216 in 84.49458503723145 seconds\n",
      "Epoch 30: total loss 104.89422810077667 in 90.25111675262451 seconds\n",
      "Epoch 40: total loss 103.32020539045334 in 94.48962211608887 seconds\n",
      "Epoch 50: total loss 106.28642398118973 in 87.42717695236206 seconds\n",
      "Epoch 60: total loss 95.63616496324539 in 90.29020571708679 seconds\n",
      "\n",
      " Teacher results:\n",
      "Test set results:\n",
      "Average case accuracy: 19177.0/20260.0=0.9465449452400208\n",
      "\n",
      "Worst case group: male [spurious] and blond_hair [target]\n",
      "Worst case accuracy: 88.0/169.0=0.5207100510597229\n",
      "\n",
      "\n",
      " Student results:\n",
      "Test set results:\n",
      "Average case accuracy: 18752.0/20260.0=0.925567626953125\n",
      "\n",
      "Worst case group: male [spurious] and blond_hair [target]\n",
      "Worst case accuracy: 121.0/169.0=0.715976357460022\n",
      "\n",
      "Starting fold 2 of 10\n",
      "---------------------\n",
      "\n",
      "Training teacher model\n",
      "Epoch 10: total loss 135.981851875782 in 88.81702470779419 seconds\n",
      "Epoch 20: total loss 71.60079246759415 in 86.29737329483032 seconds\n",
      "Epoch 30: total loss 63.14782211184502 in 85.62107515335083 seconds\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(folds):\n",
    "    results.append(run_fold(i))\n",
    "\n",
    "    with open(\"results-unaugmented-teacher-augmented-student.json\", \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher\n",
      "average: 0.9449160908193485\n",
      "worst: 0.5363064608347627\n",
      "student\n",
      "average: 0.9203849950641658\n",
      "worst: 0.8201507489743345\n"
     ]
    }
   ],
   "source": [
    "for key in [\"teacher\", \"student\"]:\n",
    "    print(key)\n",
    "\n",
    "    get = lambda k: sum([i[key][k][\"correct\"] for i in results])/sum([i[key][k][\"total\"] for i in results])\n",
    "    \n",
    "    print(f\"average: {get('average')}\")\n",
    "\n",
    "    print(f\"worst: {get('worst')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Student and Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 9 of 10\n",
      "---------------------\n",
      "restored state\n",
      "\n",
      "Training teacher model\n",
      "\n",
      "Training student models\n",
      "Epoch 10: total loss 504.9422287940979 (with distillation) 278.8648054599762 (without distillation) in 106.14713788032532 seconds\n",
      "Epoch 20: total loss 499.7467966079712 (with distillation) 277.3459746837616 (without distillation) in 96.11409091949463 seconds\n"
     ]
    }
   ],
   "source": [
    "with open(\"results-augmented-teacher-augmented-student.json\", \"r\") as f:\n",
    "    results_augmented = json.load(f)\n",
    "    \n",
    "# results_augmented = []\n",
    "folds_complete = len(results_augmented)\n",
    "\n",
    "for i in range(folds_complete, folds):\n",
    "    results_augmented.append(run_fold(i, augment_teacher=True, load=(i==folds_complete)))\n",
    "\n",
    "    with open(\"results-augmented-teacher-augmented-student.json\", \"w\") as f:\n",
    "        json.dump(results_augmented, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher\n",
      "average: 0.9432329713721619\n",
      "worst: 0.5946255002858777\n",
      "student\n",
      "average: 0.8647087857847976\n",
      "worst: 0.23943396226415095\n",
      "student_no_distillation\n",
      "average: 0.8477986179664363\n",
      "worst: 0.10624169986719788\n"
     ]
    }
   ],
   "source": [
    "for key in results_augmented[0]:\n",
    "    print(key)\n",
    "\n",
    "    get = lambda k: sum([i[key][k][\"correct\"] for i in results_augmented])/sum([i[key][k][\"total\"] for i in results_augmented])\n",
    "    \n",
    "    print(f\"average: {get('average')}\")\n",
    "    print(f\"worst: {get('worst')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'teacher': {'worst': {'correct': 129.0, 'total': 169.0},\n",
       "   'average': {'correct': 18863.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 120.0, 'total': 169.0},\n",
       "   'average': {'correct': 18869.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 138.0, 'total': 169.0},\n",
       "   'average': {'correct': 17894.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 96.0, 'total': 168.0},\n",
       "   'average': {'correct': 19140.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 121.0, 'total': 168.0},\n",
       "   'average': {'correct': 18792.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 117.0, 'total': 168.0},\n",
       "   'average': {'correct': 18940.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 76.0, 'total': 170.0},\n",
       "   'average': {'correct': 19229.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 132.0, 'total': 170.0},\n",
       "   'average': {'correct': 18489.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 119.0, 'total': 170.0},\n",
       "   'average': {'correct': 18668.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 93.0, 'total': 194.0},\n",
       "   'average': {'correct': 19208.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 134.0, 'total': 194.0},\n",
       "   'average': {'correct': 18911.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 95.0, 'total': 194.0},\n",
       "   'average': {'correct': 19081.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 97.0, 'total': 165.0},\n",
       "   'average': {'correct': 19137.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 114.0, 'total': 165.0},\n",
       "   'average': {'correct': 18661.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 127.0, 'total': 165.0},\n",
       "   'average': {'correct': 18543.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 113.0, 'total': 166.0},\n",
       "   'average': {'correct': 18997.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 80.0, 'total': 166.0},\n",
       "   'average': {'correct': 19129.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 103.0, 'total': 166.0},\n",
       "   'average': {'correct': 18917.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 111.0, 'total': 190.0},\n",
       "   'average': {'correct': 19168.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 106.0, 'total': 190.0},\n",
       "   'average': {'correct': 19040.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 73.0, 'total': 190.0},\n",
       "   'average': {'correct': 19017.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 114.0, 'total': 160.0},\n",
       "   'average': {'correct': 19035.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 123.0, 'total': 160.0},\n",
       "   'average': {'correct': 18863.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 130.0, 'total': 160.0},\n",
       "   'average': {'correct': 18941.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 110.0, 'total': 191.0},\n",
       "   'average': {'correct': 19100.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 1493.0, 'total': 9042.0},\n",
       "   'average': {'correct': 5365.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 0.0, 'total': 8231.0},\n",
       "   'average': {'correct': 2987.0, 'total': 20260.0}}},\n",
       " {'teacher': {'worst': {'correct': 101.0, 'total': 176.0},\n",
       "   'average': {'correct': 19222.0, 'total': 20260.0}},\n",
       "  'student': {'worst': {'correct': 115.0, 'total': 176.0},\n",
       "   'average': {'correct': 19071.0, 'total': 20260.0}},\n",
       "  'student_no_distillation': {'worst': {'correct': 138.0, 'total': 176.0},\n",
       "   'average': {'correct': 18776.0, 'total': 20260.0}}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerunning iteration 9\n",
    "\n",
    "Iteration 9 appears to have had a bug from incorrectly restoring from a saved state; recalculating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 9 of 10\n",
      "---------------------\n",
      "\n",
      "Training teacher model\n",
      "Epoch 10: total loss 153.7123766541481 in 86.90923738479614 seconds\n",
      "Epoch 20: total loss 115.20748114585876 in 86.31769490242004 seconds\n",
      "Epoch 30: total loss 77.30689215660095 in 87.16270160675049 seconds\n",
      "Epoch 40: total loss 42.02192668616772 in 85.42110061645508 seconds\n",
      "Epoch 50: total loss 25.61875607073307 in 88.89468908309937 seconds\n",
      "Epoch 60: total loss 2.5034093619324267 in 86.91546654701233 seconds\n",
      "\n",
      "Training student models\n",
      "Epoch 10: total loss 168.15435528755188 (with distillation) 71.89970940351486 (without distillation) in 103.67305898666382 seconds\n",
      "Epoch 20: total loss 122.86082565784454 (with distillation) 45.19628405570984 (without distillation) in 106.7306900024414 seconds\n",
      "Epoch 30: total loss 114.55890446901321 (with distillation) 39.20469355583191 (without distillation) in 103.05396866798401 seconds\n",
      "Epoch 40: total loss 138.7109397649765 (with distillation) 52.43850961327553 (without distillation) in 103.32592225074768 seconds\n",
      "Epoch 50: total loss 95.61528819799423 (with distillation) 36.55836495757103 (without distillation) in 107.34833979606628 seconds\n",
      "Epoch 60: total loss 77.03687483072281 (with distillation) 32.1574442088604 (without distillation) in 104.8129243850708 seconds\n",
      "\n",
      " Teacher results:\n",
      "Test set results:\n",
      "Average case accuracy: 19067.0/20260.0=0.9411154985427856\n",
      "\n",
      "Worst case group: male [spurious] and blond_hair [target]\n",
      "Worst case accuracy: 110.0/191.0=0.5759162306785583\n",
      "\n",
      "\n",
      " Student results:\n",
      "Test set results:\n",
      "Average case accuracy: 18688.0/20260.0=0.9224086999893188\n",
      "\n",
      "Worst case group: male [spurious] and blond_hair [target]\n",
      "Worst case accuracy: 155.0/191.0=0.8115183115005493\n",
      "\n",
      "\n",
      " Student results (without distillation):\n",
      "Test set results:\n",
      "Average case accuracy: 18749.0/20260.0=0.9254195690155029\n",
      "\n",
      "Worst case group: male [spurious] and blond_hair [target]\n",
      "Worst case accuracy: 139.0/191.0=0.727748692035675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_augmented[8] = run_fold(8, augment_teacher=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher\n",
      "average: 0.9430700888450148\n",
      "worst: 0.5946255002858777\n",
      "student\n",
      "average: 0.9304689042448173\n",
      "worst: 0.6861063464837049\n",
      "student_no_distillation\n",
      "average: 0.9255972359328727\n",
      "worst: 0.6740994854202401\n"
     ]
    }
   ],
   "source": [
    "for key in results_augmented[0]:\n",
    "    print(key)\n",
    "\n",
    "    get = lambda k: sum([i[key][k][\"correct\"] for i in results_augmented])/sum([i[key][k][\"total\"] for i in results_augmented])\n",
    "    \n",
    "    print(f\"average: {get('average')}\")\n",
    "    print(f\"worst: {get('worst')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results-augmented-teacher-augmented-student.json\", \"w\") as f:\n",
    "        json.dump(results_augmented, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unaugmented Student and Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fold 4 of 10\n",
      "---------------------\n",
      "restored state\n",
      "\n",
      "Training teacher model\n",
      "\n",
      "Training student models\n",
      "Epoch 10: total loss 504.7459120750427 (with distillation) 250.44861817359924 (without distillation) in 105.20854139328003 seconds\n",
      "Epoch 20: total loss 504.5829710960388 (with distillation) 250.48265051841736 (without distillation) in 106.46074628829956 seconds\n",
      "Epoch 30: total loss 506.7782826423645 (with distillation) 251.40503478050232 (without distillation) in 100.30206108093262 seconds\n",
      "Epoch 40: total loss 504.2388153076172 (with distillation) 250.2957627773285 (without distillation) in 102.28609013557434 seconds\n",
      "Epoch 50: total loss 504.8196077346802 (with distillation) 250.55920815467834 (without distillation) in 104.39407134056091 seconds\n",
      "Epoch 60: total loss 507.2005515098572 (with distillation) 251.6191053390503 (without distillation) in 113.73196864128113 seconds\n",
      "\n",
      " Teacher results:\n",
      "Test set results:\n",
      "Average case accuracy: 17621.0/20260.0=0.8697433471679688\n",
      "\n",
      "Worst case group: not male [spurious] and not blond_hair [target]\n",
      "Worst case accuracy: 7541.0/8978.0=0.8399420976638794\n",
      "\n",
      "\n",
      " Student results:\n",
      "Test set results:\n",
      "Average case accuracy: 3362.0/20260.0=0.1659427434206009\n",
      "\n",
      "Worst case group: not male [spurious] and not blond_hair [target]\n",
      "Worst case accuracy: 192.0/8978.0=0.021385610103607178\n",
      "\n",
      "\n",
      " Student results (without distillation):\n",
      "Test set results:\n",
      "Average case accuracy: 3879.0/20260.0=0.1914610117673874\n",
      "\n",
      "Worst case group: not male [spurious] and not blond_hair [target]\n",
      "Worst case accuracy: 436.0/8978.0=0.048563152551651\n",
      "\n",
      "Starting fold 5 of 10\n",
      "---------------------\n",
      "\n",
      "Training teacher model\n",
      "Epoch 10: total loss 134.39596316218376 in 86.42588067054749 seconds\n",
      "Epoch 20: total loss 64.05533027648926 in 85.25792098045349 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7654d3fdf501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresults_unaugmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_student\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mfolds_complete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results-unaugmented-teacher-unaugmented-student.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-adbf324a8668>\u001b[0m in \u001b[0;36mrun_fold\u001b[0;34m(fold, permuted_indices, augment_teacher, augment_student, load)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6ef6c5cd1186>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(logits, labels, groups)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_fractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mgroup_fractions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"results-unaugmented-teacher-unaugmented-student.json\", \"r\") as f:\n",
    "    results_unaugmented = json.load(f)\n",
    "\n",
    "# results_unaugmented = []\n",
    "folds_complete = len(results_unaugmented)\n",
    "\n",
    "for i in range(folds_complete, folds):\n",
    "    results_unaugmented.append(run_fold(i, augment_student=False, load=(i==folds_complete)))\n",
    "\n",
    "    with open(\"results-unaugmented-teacher-unaugmented-student.json\", \"w\") as f:\n",
    "        json.dump(results_unaugmented, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:distillation]",
   "language": "python",
   "name": "conda-env-distillation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
